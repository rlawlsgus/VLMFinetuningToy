{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb93982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb4501f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu129\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.23.0+cu129)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./venv/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./venv/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.12/site-packages (from ipywidgets) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ## 1. 라이브러리 설치 및 로그인\n",
    "\n",
    "# Octo 모델 및 TRL 최신 버전 설치\n",
    "%pip install -U -q git+https://github.com/huggingface/trl.git transformers datasets accelerate peft trl bitsandbytes einops\n",
    "\n",
    "%pip install pandas\n",
    "\n",
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu129\n",
    "# %pip install hf_xet\n",
    "\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03802720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc846fd3e0b44d67be0657fc11b0c77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hugging Face Hub 로그인\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55279c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. 라이브러리 임포트 및 기본 설정\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2253255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples created: 50172\n",
      "Example samples:\n",
      "{'observation_images': ['/home/rlawlsgus/github/VLMFinetuningToy/Assets_Zara01/FoVImages/zara01_146/t_0.jpg'], 'instruction': 'From the start position [17.03, 0.00, 2.57], reach the goal position [-1.32, 0.00, 2.60].', 'action': [-143.94, 1.8039]}\n",
      "{'observation_images': ['/home/rlawlsgus/github/VLMFinetuningToy/Assets_Zara01/FoVImages/zara01_146/t_1.jpg'], 'instruction': 'From the start position [17.03, 0.00, 2.57], reach the goal position [-1.32, 0.00, 2.60].', 'action': [-143.94, 1.8039]}\n",
      "{'observation_images': ['/home/rlawlsgus/github/VLMFinetuningToy/Assets_Zara01/FoVImages/zara01_146/t_2.jpg'], 'instruction': 'From the start position [17.03, 0.00, 2.57], reach the goal position [-1.32, 0.00, 2.60].', 'action': [-143.94, 1.8029]}\n",
      "{'observation_images': ['/home/rlawlsgus/github/VLMFinetuningToy/Assets_Zara01/FoVImages/zara01_146/t_3.jpg'], 'instruction': 'From the start position [17.03, 0.00, 2.57], reach the goal position [-1.32, 0.00, 2.60].', 'action': [-143.9438, 1.8039]}\n",
      "{'observation_images': ['/home/rlawlsgus/github/VLMFinetuningToy/Assets_Zara01/FoVImages/zara01_146/t_4.jpg'], 'instruction': 'From the start position [17.03, 0.00, 2.57], reach the goal position [-1.32, 0.00, 2.60].', 'action': [-143.94, 1.8039]}\n"
     ]
    }
   ],
   "source": [
    "# ## 3. 데이터셋 준비\n",
    "#\n",
    "# **중요! 수정된 부분**\n",
    "#\n",
    "# - `Assets_Zara01` 폴더의 CSV 파일과 이미지 데이터를 로드하도록 수정되었습니다.\n",
    "# - 각 CSV 파일은 하나의 에피소드(trajectory)를 나타냅니다.\n",
    "# - 각 샘플은 (현재 이미지, 자연어 명령, 현재 액션)으로 구성됩니다.\n",
    "# - **자연어 명령을 CSV의 시작/목표 위치를 사용하여 동적으로 생성하도록 변경했습니다.**\n",
    "# - 액션은 `[Action_X, Action_Z]` 2차원 벡터를 사용합니다.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 현재 작업 디렉토리 (프로젝트 루트)\n",
    "project_root = os.getcwd()\n",
    "data_root = os.path.join(project_root, \"Assets_Zara01\")\n",
    "state_files_path = os.path.join(data_root, \"Zara01_State\")\n",
    "\n",
    "# 모든 CSV 파일 경로 가져오기\n",
    "csv_files = glob.glob(os.path.join(state_files_path, \"*.csv\"))\n",
    "\n",
    "raw_dataset = []\n",
    "\n",
    "# 각 CSV 파일(에피소드)을 순회\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    if df.empty:\n",
    "        continue\n",
    "        \n",
    "    # 에피소드의 시작 위치와 목표 위치 추출 (첫 번째 행 기준)\n",
    "    start_pos_row = df.iloc[0]\n",
    "    start_pos = [start_pos_row['Position_X'], start_pos_row['Position_Y'], start_pos_row['Position_Z']]\n",
    "    goal_pos = [start_pos_row['GoalPosition_X'], start_pos_row['GoalPosition_Y'], start_pos_row['GoalPosition_Z']]\n",
    "    \n",
    "    # 영어로 자연어 명령 생성 (소수점 2자리까지)\n",
    "    instruction = (\n",
    "        f\"From the start position [{start_pos[0]:.2f}, {start_pos[1]:.2f}, {start_pos[2]:.2f}], \"\n",
    "        f\"reach the goal position [{goal_pos[0]:.2f}, {goal_pos[1]:.2f}, {goal_pos[2]:.2f}].\"\n",
    "    )\n",
    "    \n",
    "    # 에피소드의 각 타임스텝을 순회하며 데이터 샘플 생성\n",
    "    for index, row in df.iterrows():\n",
    "        # CSV에 있는 이미지 경로는 'Assets/...'로 시작하는 상대 경로입니다.\n",
    "        # os.path.abspath를 사용하여 현재 작업 디렉토리 기준으로 절대 경로를 생성합니다.\n",
    "        relative_image_path = row['FovImagePath'].replace('/', os.sep).replace('Assets', 'Assets_Zara01')\n",
    "        image_path = os.path.abspath(relative_image_path)\n",
    "\n",
    "        # 액션 데이터 추출\n",
    "        action = [row['Action_X'], row['Action_Z']]\n",
    "        \n",
    "        # 데이터 샘플 생성\n",
    "        raw_dataset.append({\n",
    "            \"observation_images\": [image_path],\n",
    "            \"instruction\": instruction, \n",
    "            \"action\": action,\n",
    "        })\n",
    "\n",
    "# 데이터셋 크기 확인 (너무 많으면 일부만 사용)\n",
    "print(f\"Total samples created: {len(raw_dataset)}\")\n",
    "# 예시로 처음 5개 샘플 출력\n",
    "print(\"Example samples:\")\n",
    "for i in range(min(5, len(raw_dataset))):\n",
    "    print(raw_dataset[i])\n",
    "\n",
    "# Hugging Face Dataset 객체로 변환\n",
    "# 전체 데이터가 너무 클 경우, 메모리 부족을 방지하기 위해 일부만 사용할 수 있습니다.\n",
    "# 예: hf_dataset = Dataset.from_list(raw_dataset[:1000])\n",
    "hf_dataset = Dataset.from_list(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b589985",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in rail-berkeley/octo-small-1.5. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, apertus, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, dinov3_convnext, dinov3_vit, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, florence2, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_moe, glm4v_moe_text, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, hunyuan_v1_dense, hunyuan_v1_moe, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kosmos-2.5, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, metaclip_2, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, ovis2, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam2, sam2_hiera_det_model, sam2_video, sam2_vision_model, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, seed_oss, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xcodec, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mrail-berkeley/octo-small-1.5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 모델과 프로세서 로드\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mAutoModelForImageTextToText\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# model_type 오류 해결\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/VLMFinetuningToy/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:549\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    547\u001b[39m     _ = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m config, kwargs = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig.get(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/VLMFinetuningToy/venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1329\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1326\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[32m   1327\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern].from_dict(config_dict, **unused_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1330\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1331\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, or contain one of the following strings \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(CONFIG_MAPPING.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1333\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized model in rail-berkeley/octo-small-1.5. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, apertus, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, dinov3_convnext, dinov3_vit, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, florence2, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_moe, glm4v_moe_text, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, hunyuan_v1_dense, hunyuan_v1_moe, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kosmos-2.5, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, metaclip_2, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, ovis2, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam2, sam2_hiera_det_model, sam2_video, sam2_vision_model, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, seed_oss, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xcodec, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth"
     ]
    }
   ],
   "source": [
    "# ## 4. 모델 로드\n",
    "# - BitsAndBytesConfig를 사용하여 4비트 양자화로 모델을 로드하여 메모리를 절약합니다.\n",
    "# - `AutoModelForVision2Seq`는 지원 중단 예정이므로 권장되는 `AutoModelForImageTextToText`를 사용합니다.\n",
    "# - `trust_remote_code=True`를 설정하여 Hugging Face Hub의 모델 코드를 직접 실행하도록 허용합니다.\n",
    "\n",
    "from transformers import AutoModelForImageTextToText\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 사용할 Octo 모델 ID\n",
    "model_id = \"rail-berkeley/octo-small-1.5\"\n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True  # model_type 오류 해결\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. 데이터 포맷팅 함수 정의\n",
    "# \n",
    "# **중요! 수정이 필요한 부분**\n",
    "# \n",
    "# - `SFTTrainer`에 데이터를 올바르게 전달하기 위한 함수입니다.\n",
    "# - Octo는 `text`와 `images`를 입력으로 받습니다. `text`는 `processor.tokenizer.apply_chat_template`을 사용하여 생성합니다.\n",
    "# - `action`을 정규화하고 토큰화하는 과정이 필요할 수 있습니다. 이 부분은 Octo의 공식 예제를 참고하여 데이터에 맞게 조정해야 합니다.\n",
    "\n",
    "def format_for_octo(sample):\n",
    "    # 1. 이미지 로드\n",
    "    images = [Image.open(path).convert(\"RGB\") for path in sample[\"observation_images\"]]\n",
    "    \n",
    "    # 2. 텍스트(명령)를 대화 템플릿에 맞게 변환\n",
    "    # Octo는 특정 대화 형식을 따릅니다.\n",
    "    text = processor.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"<image>\\n{sample['instruction']}\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 3. 모델 입력 생성\n",
    "    inputs = processor(text=text, images=images, return_tensors=\"pt\")\n",
    "    \n",
    "    # 4. 레이블(정답 액션) 처리\n",
    "    # 중요! 액션 값을 모델 출력에 맞게 변환해야 합니다.\n",
    "    # Octo는 액션을 이산적인 토큰으로 예측하므로, 연속적인 액션 값을 binning(양동이질)하는 과정이 필요합니다.\n",
    "    # 아래는 간단한 예시이며, 실제로는 데이터의 분포를 보고 bin 경계를 정해야 합니다.\n",
    "    action_bins = torch.linspace(-1.0, 1.0, 256) # -1에서 1 사이를 256개 구간으로 나눔\n",
    "    \n",
    "    # numpy 배열을 torch 텐서로 변환\n",
    "    action_tensor = torch.tensor(sample[\"action\"], dtype=torch.float32)\n",
    "    \n",
    "    # 각 액션 차원 값을 가장 가까운 bin의 인덱스(토큰 ID)로 변환\n",
    "    # 이 부분이 Octo 학습의 핵심입니다!\n",
    "    action_labels = torch.bucketize(action_tensor, action_bins)\n",
    "    \n",
    "    # SFTTrainer는 labels가 input_ids와 같은 길이를 기대하는 경우가 많습니다.\n",
    "    # 여기서는 입력 텍스트 부분은 무시하고(-100), 액션 부분만 학습하도록 레이블을 생성합니다.\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    labels = torch.full((1, input_len + len(action_labels)), -100)\n",
    "    labels[0, input_len:] = action_labels\n",
    "    \n",
    "    # input_ids와 attention_mask에 액션 레이블을 이어붙입니다.\n",
    "    # 이 부분은 모델 아키텍처나 학습 방식에 따라 달라질 수 있습니다.\n",
    "    # 가장 간단한 방식은 SFTTrainer가 텍스트 생성처럼 처리하도록 하는 것입니다.\n",
    "    # 이 경우, 액션 토큰을 텍스트의 일부처럼 이어붙여야 합니다.\n",
    "    \n",
    "    # 여기서는 간단하게 input_ids와 labels만 반환하고,\n",
    "    # SFTTrainer의 data_collator가 처리하도록 위임하는 방식을 가정합니다.\n",
    "    # 더 정확한 구현을 위해서는 Octo의 공식 학습 스크립트를 참고하는 것이 좋습니다.\n",
    "    \n",
    "    # 이 스켈레톤에서는 SFTTrainer에 직접 딕셔너리를 전달하기 위해\n",
    "    # 데이터셋 자체를 변환하는 방식을 사용합니다.\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"images\": images,\n",
    "        \"labels\": action_labels.tolist() # 리스트로 변환하여 데이터셋에 저장\n",
    "    }\n",
    "\n",
    "# 데이터셋 변환 (이 방식은 메모리를 많이 사용할 수 있습니다. 큰 데이터셋은 map을 사용하세요)\n",
    "# processed_dataset = hf_dataset.map(format_for_octo, remove_columns=list(hf_dataset.features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea77d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. PEFT (LoRA) 설정\n",
    "# - 기존 코드와 유사하게 LoRA를 설정하여 효율적인 파인튜닝을 합니다.\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"VISION_2_SEQ_MODEL\", # Octo와 같은 모델은 이 타입을 사용\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# get_peft_model 함수는 더 이상 명시적으로 필요하지 않을 수 있습니다.\n",
    "# Trainer가 내부적으로 처리합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. SFTTrainer 설정 및 학습\n",
    "# - `SFTConfig`를 사용하여 학습 파라미터를 정의합니다.\n",
    "# - `SFTTrainer`에 모델, 데이터셋, PEFT 설정 등을 전달하여 학습을 시작합니다.\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"octo-small-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2, # GPU 메모리에 따라 조절\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\", # 평가 데이터셋이 없다면 no\n",
    "    bf16=True,\n",
    "    # --- SFTTrainer를 위한 추가 인수 ---\n",
    "    dataset_text_field=\"text\", # 데이터셋에서 텍스트 필드를 지정\n",
    "    max_seq_length=1024, # 최대 시퀀스 길이\n",
    "    # packing=True, # 여러 짧은 샘플을 묶어 효율성 증대\n",
    ")\n",
    "\n",
    "# SFTTrainer 초기화\n",
    "# 중요!: SFTTrainer는 기본적으로 텍스트 데이터를 처리합니다.\n",
    "# Vision-Language 모델을 위해서는 커스텀 collator를 제공하거나,\n",
    "# TRL의 최신 기능이 processor를 통해 이미지 처리를 지원하는지 확인해야 합니다.\n",
    "# 아래 코드는 processor를 직접 trainer에 전달하여 이미지 처리를 위임하는 방식입니다.\n",
    "\n",
    "# TRL이 processor를 내부적으로 사용하여 이미지를 텐서로 변환하도록 함\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset, # 전처리되지 않은 원본 데이터셋 전달\n",
    "    dataset_map_function=format_for_octo, # map 함수를 트레이너에 위임 (메모리 효율적)\n",
    "    peft_config=peft_config,\n",
    "    processor=processor, # Processor를 전달\n",
    ")\n",
    "\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(\"./octo-small-finetuned/final_checkpoint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
